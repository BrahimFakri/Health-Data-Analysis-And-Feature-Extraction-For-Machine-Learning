{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Imports :"
      ],
      "metadata": {
        "id": "6hD8J1ti1jnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if package not installed, install with command:\n",
        "#pip install torchxrayvision"
      ],
      "metadata": {
        "id": "K7oSMiqKH6_B"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchxrayvision as xrv\n",
        "\n",
        "import skimage\n",
        "#Image Processing for Python\n",
        "#scikit-image (a.k.a. skimage) is a collection of algorithms for image processing and computer vision\n",
        "\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "#The torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors.\n",
        "\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "xqLQi5hCHzsv"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import a chest xray image"
      ],
      "metadata": {
        "id": "9_PXuVVe2gsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = skimage.io.imread('/content/02aa804e-bde0afdd-112c0b34-7bc16630-4e384014.jpg')"
      ],
      "metadata": {
        "id": "CB0hqP_cINd3"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Display image details\n",
        "*the image is read as an array (type = numpy.ndarray)*"
      ],
      "metadata": {
        "id": "Cwp57PGl24Ny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img"
      ],
      "metadata": {
        "id": "4qGbnxRfKdZz",
        "outputId": "18d28984-2d07-469e-fceb-73cbc945b552",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Select model to use for feature extraction\n",
        "*we choose densenet121-res224-chex as the model weights name from Stanford university CheXpert models*"
      ],
      "metadata": {
        "id": "PJEfChUn3PYr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "ZoLzhnyXGZuh"
      },
      "outputs": [],
      "source": [
        "\n",
        "model_weights_name = \"densenet121-res224-chex\"\n",
        "\n",
        "#Initialize the densefeature_embeddings with an empty list:\n",
        "densefeature_embeddings = []\n",
        "\n",
        "#Initialize the prediction_embeddings with an empty list:\n",
        "prediction_embeddings = []\n",
        "\n",
        "#Scales images to be roughly [-1024 1024]:\n",
        "img = xrv.datasets.normalize(img, 255)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For each image check if they are 2D arrays\n",
        "if len(img.shape) > 2:\n",
        "    img = img[:, :, 0]\n",
        "if len(img.shape) < 2:\n",
        "    print(\"Error: Dimension lower than 2 for image!\")"
      ],
      "metadata": {
        "id": "DK5Vf76xZWRf"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add color channel for prediction\n",
        "#Resize using OpenCV\n",
        "img = cv2.resize(img, (224, 224), interpolation = cv2.INTER_AREA)   \n",
        "img = img[None, :, :]"
      ],
      "metadata": {
        "id": "eZbAB_Fhdfzu"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select model \n",
        "model = xrv.models.DenseNet(weights = model_weights_name)\n"
      ],
      "metadata": {
        "id": "NGLdEFEJiV_J"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Dnz0Rikifea",
        "outputId": "bbce4d7b-54ab-4503-a0d4-4d7a82588564"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XRV-DenseNet121-densenet121-res224-chex"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select if you want to use CUDA support for GPU (optional as it is usually pretty fast even in CPUT)\n",
        "cuda = False"
      ],
      "metadata": {
        "id": "4Pwr2zXAQAKY"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img"
      ],
      "metadata": {
        "id": "t5Lzhqg3HXnl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef563963-13e4-4436-8e8d-dcf917463911"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[-1024.0001, -1024.0001, -1024.0001,  ..., -1024.0001,\n",
              "           -1024.0001, -1024.0001],\n",
              "          [-1024.0001, -1024.0001, -1024.0001,  ..., -1024.0001,\n",
              "           -1024.0001, -1024.0001],\n",
              "          [-1024.0000, -1024.0000, -1024.0000,  ..., -1024.0000,\n",
              "           -1024.0000, -1024.0000],\n",
              "          ...,\n",
              "          [-1024.0001, -1024.0001, -1024.0001,  ..., -1024.0001,\n",
              "           -1024.0001, -1024.0001],\n",
              "          [-1024.0001, -1024.0001, -1024.0001,  ..., -1024.0001,\n",
              "           -1024.0001, -1024.0001],\n",
              "          [-1024.0001, -1024.0001, -1024.0001,  ..., -1024.0001,\n",
              "           -1024.0001, -1024.0001]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = {}\n",
        "with torch.no_grad():\n",
        "    img = torch.from_numpy(img).unsqueeze(0)\n",
        "    if cuda:\n",
        "        img = img.cuda()\n",
        "        model = model.cuda()\n",
        "      \n",
        "    # Extract dense features\n",
        "    feats = model.features(img)\n",
        "    feats = F.relu(feats, inplace=True)\n",
        "    feats = F.adaptive_avg_pool2d(feats, (1, 1))\n",
        "    densefeatures = feats.cpu().detach().numpy().reshape(-1)\n",
        "    densefeature_embeddings = densefeatures\n",
        "\n",
        "    # Extract predicted probabilities of considered 18 classes:\n",
        "    # ['Atelectasis','Consolidation','Infiltration','Pneumothorax','Edema','Emphysema',Fibrosis',\n",
        "    #  'Effusion','Pneumonia','Pleural_Thickening','Cardiomegaly','Nodule',Mass','Hernia',\n",
        "    #  'Lung Lesion','Fracture','Lung Opacity','Enlarged Cardiomediastinum']\n",
        "    preds = model(img).cpu()\n",
        "    predictions = preds[0].detach().numpy()\n",
        "    prediction_embeddings = predictions  \n",
        "\n",
        "# Return embeddings\n",
        "#return densefeature_embeddings, prediction_embeddings"
      ],
      "metadata": {
        "id": "Ux4MDw8lGtOF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6582b7b5-3b6b-4f96-d615-63454c2a182b"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Input image does not appear to be normalized correctly. The input image has the range [-1024.00,1019.87] which doesn't seem to be in the [-1024,1024] range. This warning may be wrong though. Only the first image is tested and we are only using a heuristic in an attempt to save a user from using the wrong normalization.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(preds)\n",
        "len(preds)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeyKhGrxHmRw",
        "outputId": "8bfa74e1-b0a5-4890-8256-fd65263e4ea0"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2119, 0.1532, 0.5000, 0.0455, 0.2962, 0.5000, 0.5000, 0.0979, 0.1885,\n",
            "         0.5000, 0.1239, 0.5000, 0.5000, 0.5000, 0.2773, 0.4033, 0.3089, 0.1374]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predictions)\n",
        "len(predictions)"
      ],
      "metadata": {
        "id": "_gCJMy66s7Ay",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0749d932-04e0-4637-c3f5-e4d7e422e42e"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.21186149 0.15321094 0.5        0.04549948 0.29623356 0.5\n",
            " 0.5        0.09788145 0.1885436  0.5        0.12390422 0.5\n",
            " 0.5        0.5        0.2773131  0.4033455  0.3089283  0.13744338]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(prediction_embeddings)\n",
        "len(prediction_embeddings)"
      ],
      "metadata": {
        "id": "igoxavM0s7D3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ef48309-1619-45aa-dff3-650e807057be"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.21186149 0.15321094 0.5        0.04549948 0.29623356 0.5\n",
            " 0.5        0.09788145 0.1885436  0.5        0.12390422 0.5\n",
            " 0.5        0.5        0.2773131  0.4033455  0.3089283  0.13744338]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(densefeature_embeddings)\n",
        "len(densefeature_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "as-OPYebEkhU",
        "outputId": "2a71aee6-74b0-4444-9ae3-93fc77c49b0d"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.         0.         0.1838883  ... 0.00537282 0.         0.10955384]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1024"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nv5WVe5IGtg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yVTSHwMGGtjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NCO8L9t4GtmU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
